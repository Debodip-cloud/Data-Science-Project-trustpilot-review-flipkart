# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SS_WG8TXUULk5qZIWWepMAghmB4tvs92
"""

import pandas as pd

# Load the CSV file
df = pd.read_csv("trustpilotFlipkartReviews.csv")

# Dropping rows where at least one column is missing
df_cleaned = df.dropna(subset=["Title", "Content", "Rating"])


print(df_cleaned.head())



import spacy
import re

nlp = spacy.load("en_core_web_sm")

def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r"http\S+|www\S+", "", text)  # Remove URLs
    text = re.sub(r"[^\w\s]", "", text)  # Remove punctuation
    doc = nlp(text)
    tokens = [token.lemma_ for token in doc if not token.is_stop]  # Lemmatization & stopword removal
    return " ".join(tokens)

df_cleaned["Content"] = df_cleaned["Content"].apply(clean_text)
df_cleaned["Title"] = df_cleaned["Title"].apply(clean_text)

from collections import Counter
import matplotlib.pyplot as plt

words = " ".join(df_cleaned["Content"]).split()
word_freq = Counter(words).most_common(20)  # Top 20 words

plt.figure(figsize=(10, 5))
plt.bar(*zip(*word_freq))
plt.xticks(rotation=45)
plt.title("Most Common Words in Reviews")
plt.show()

from collections import Counter
import matplotlib.pyplot as plt

words = " ".join(df_cleaned["Title"]).split()
word_freq = Counter(words).most_common(20)  # Top 20 words

plt.figure(figsize=(10, 5))
plt.bar(*zip(*word_freq))
plt.xticks(rotation=45)
plt.title("Most Common Words in Reviews")
plt.show()

from textblob import TextBlob

df_cleaned["sentiment"] = df_cleaned["Content"].apply(lambda x: TextBlob(x).sentiment.polarity)
df_cleaned["sentiment_label"] = df_cleaned["sentiment"].apply(lambda x: "positive" if x > 0 else ("negative" if x < 0 else "neutral"))
df_cleaned["sentiment_label"].value_counts().plot(kind="bar")

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
df_cleaned["tokenized_content"] = df_cleaned["Content"].apply(lambda x: tokenizer.encode(x, truncation=True, padding="max_length", max_length=128))
df_cleaned["tokenized_title"] = df_cleaned["Title"].apply(lambda x: tokenizer.encode(x, truncation=True, padding="max_length", max_length=128))

print(df_cleaned.dtypes)

pip install pymongo

from pymongo import MongoClient

# Connect to MongoDB
client = MongoClient("______")  # Replace with your MongoDB URI if using Atlas
try:
    client.admin.command('ping')
    print("Connected to MongoDB Atlas!")
except Exception as e:
    print("Error:", e)
db = client["trustpilot_reviews_db"]
collection = db["reviews"]

# Insert Data
for _, row in df_cleaned.iterrows():
    review_doc = {
        "Content": row["Content"],
        "cleaned_text": row["Title"],
        "sentiment": row["sentiment"],
        "sentiment_label": row["sentiment_label"],
        "tokenized_content": row["tokenized_content"],
        "tokenized_title": row["tokenized_title"]
    }
    collection.insert_one(review_doc)

print("Data stored successfully!")

"""reviews = collection.find({"sentiment_label": "negative"})  # Get only negative reviews

for review in reviews:
    print(review["cleaned_text"])
"""

client.close()

!pip install transformers torch pandas sklearn

# Sentiment Analysis with DistilBERT (Optimized)
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
import numpy as np
import torch

# Load and split data
texts = df_cleaned["Content"].tolist()
labels = df_cleaned["sentiment_label"].tolist()
train_texts, test_texts, train_labels, test_labels = train_test_split(
    texts, labels, test_size=0.2, random_state=42
)

# Encode labels
le = LabelEncoder()
train_labels = le.fit_transform(train_labels)
test_labels = le.transform(test_labels)

# Initialize DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

# Tokenize datasets
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)

# Dataset Class
class SentimentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Create datasets
train_dataset = SentimentDataset(train_encodings, train_labels)
test_dataset = SentimentDataset(test_encodings, test_labels)

# Initialize DistilBERT model
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=len(le.classes_)
)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=100,
    save_steps=500,
    fp16=True,  # Enable mixed precision training
    load_best_model_at_end=True,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# Train the model
trainer.train()

# Evaluate
predictions = trainer.predict(test_dataset)
preds = np.argmax(predictions.predictions, axis=1)
print(f"\nTest Accuracy: {accuracy_score(test_labels, preds):.2%}")

# Prediction function
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return le.inverse_transform([torch.argmax(outputs.logits)])[0]

# Test prediction
sample_text = "This product works great and exceeded my expectations!"
print(f"\nPrediction for '{sample_text}': {predict_sentiment(sample_text)}")

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import numpy as np

# 1. Data Preparation ----------------------------------------------
# Sample reduction for faster testing (remove in production)
df = df_cleaned.sample(2000, random_state=42) if len(df_cleaned) > 5000 else df_cleaned

# Label encoding
le = LabelEncoder()
df['encoded_labels'] = le.fit_transform(df['sentiment_label'])

# Train-test split
train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['Content'].tolist(),
    df['encoded_labels'].tolist(),
    test_size=0.2,
    random_state=42
)

# 2. Tokenization --------------------------------------------------
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Tokenize in batches (faster)
def batch_tokenize(texts, batch_size=32):
    return [tokenizer(text, truncation=True, padding=True, max_length=64) for text in texts]  # Reduced max_length

train_encodings = batch_tokenize(train_texts)
test_encodings = batch_tokenize(test_texts)

# 3. Dataset Class -------------------------------------------------
class SentimentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = SentimentDataset(train_encodings, train_labels)
test_dataset = SentimentDataset(test_encodings, test_labels)

# 4. Model Setup ---------------------------------------------------
model = DistilBertForSequenceClassification.from_pretrained(
    'distilbert-base-uncased',
    num_labels=len(le.classes_)
).to('cuda' if torch.cuda.is_available() else 'cpu')

# 5. Optimized Training --------------------------------------------
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=2,                     # Reduced epochs
    per_device_train_batch_size=32,         # Increased batch size
    per_device_eval_batch_size=64,
    evaluation_strategy='steps',
    eval_steps=100,
    save_steps=500,
    fp16=True,                              # Mixed precision
    load_best_model_at_end=True,
    logging_dir='./logs',
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# 6. Training & Evaluation -----------------------------------------
print("Starting training...")
trainer.train()

# Predictions
predictions = trainer.predict(test_dataset)
preds = np.argmax(predictions.predictions, axis=-1)

# Decode labels
test_labels_decoded = le.inverse_transform(test_labels)
preds_decoded = le.inverse_transform(preds)

print("\nClassification Report:")
print(classification_report(test_labels_decoded, preds_decoded))

# 7. Prediction Function -------------------------------------------
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=64).to(model.device)
    with torch.no_grad():
        outputs = model(**inputs)
    return le.inverse_transform([torch.argmax(outputs.logits).item()])[0]

# Test Cases
test_phrases = [
    "This product works perfectly!",
    "Worst customer service ever",
    "It's okay, not great"
]

for phrase in test_phrases:
    print(f"\nText: {phrase}")
    print(f"Prediction: {predict_sentiment(phrase)}")

import pandas as pd
import fasttext
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# 1. Prepare Data ---------------------------------------------------
# Convert to FastText format (__label__prefix required)
df_cleaned['fasttext_format'] = df_cleaned.apply(
    lambda row: f"__label__{row['sentiment_label']} {row['Content']}",
    axis=1
)

# Split data (80% train, 20% test)
train, test = train_test_split(df_cleaned, test_size=0.2, random_state=42)

# Save to text files (FastText requires file input)
train[['fasttext_format']].to_csv('fasttext_train.txt',
                                 index=False,
                                 header=False,
                                 sep='\n')
test[['fasttext_format']].to_csv('fasttext_test.txt',
                                index=False,
                                header=False,
                                sep='\n')

# 2. Train Model ---------------------------------------------------
model = fasttext.train_supervised(
    input='fasttext_train.txt',
    epoch=50,            # Number of iterations
    lr=0.5,              # Learning rate
    wordNgrams=2,        # Uses word pairs (bigrams)
    dim=100,             # Embedding dimension
    loss='ova'           # One-vs-all for multi-class
)

# 3. Evaluate ------------------------------------------------------
def fasttext_predict(text):
    # Convert text to list before prediction
    labels, probs = model.predict([text.replace('\n', ' ')], k=1)
    # Return the first predicted label, removing '__label__' prefix
    return labels[0][0].replace('__label__', '')

# Get predictions
test['predicted_label'] = test['Content'].apply(fasttext_predict)

# Classification report
print(classification_report(test['sentiment_label'],
                           test['predicted_label']))

# 4. Save & Load Model ---------------------------------------------
model.save_model('sentiment_model.ftz')

# Later load with:
# model = fasttext.load_model('sentiment_model.ftz')

# 5. Make Predictions ----------------------------------------------
sample_texts = [
    "This product works perfectly!",
    "Worst customer service ever",
    "It's okay, not great"
]

for text in sample_texts:
    print(f"Text: {text}")
    print(f"Prediction: {fasttext_predict(text)}\n")

!pip install fasttext

import pandas as pd
import fasttext
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# 1. DATA PREPARATION ----------------------------------------------
# Clean text (critical for FastText)
def clean_text(text):
    text = str(text).lower()
    text = text.replace('\n', ' ').replace('\r', '')
    return ' '.join(text.split())

df_cleaned['cleaned_content'] = df_cleaned['Content'].apply(clean_text)

# Balance classes (undersample negative)
negative_samples = df_cleaned[df_cleaned['sentiment_label'] == 'negative']
neutral_samples = df_cleaned[df_cleaned['sentiment_label'] == 'neutral']
positive_samples = df_cleaned[df_cleaned['sentiment_label'] == 'positive']

# Take min samples per class (or use oversampling)
min_samples = min(len(negative_samples), len(neutral_samples), len(positive_samples))
balanced_df = pd.concat([
    negative_samples.sample(min_samples, random_state=42),
    neutral_samples.sample(min_samples, random_state=42),
    positive_samples.sample(min_samples, random_state=42)
])

# 2. MODEL TRAINING ------------------------------------------------
# Format for FastText
balanced_df['fasttext_format'] = balanced_df.apply(
    lambda row: f"__label__{row['sentiment_label']} {row['cleaned_content']}",
    axis=1
)

# Split data
train, test = train_test_split(balanced_df, test_size=0.2, random_state=42)

# Save files
train[['fasttext_format']].to_csv('balanced_train.txt', index=False, header=False, sep='\n')
test[['fasttext_format']].to_csv('balanced_test.txt', index=False, header=False, sep='\n')

# Hyperparameter tuning
model = fasttext.train_supervised(
    input='balanced_train.txt',
    epoch=100,            # More iterations
    lr=0.05,             # Lower learning rate
    wordNgrams=3,        # Better context capture
    loss='ova',          # One-vs-all for multi-class
    minCount=2,          # Ignore rare words
    bucket=200000        # More hash buckets
)

# 3. EVALUATION ---------------------------------------------------
def predict_sentiment(text):
    text = clean_text(text)
    labels, probs = model.predict([text.replace('\n', ' ')], k=1)
    return labels[0][0].replace('__label__', '')

test['prediction'] = test['cleaned_content'].apply(predict_sentiment)
print(classification_report(test['sentiment_label'], test['prediction']))

# 4. TEST CASES ---------------------------------------------------
test_phrases = [
    "This product works perfectly!",
    "Worst customer service ever",
    "It's okay, not great",
    "Absolutely fantastic experience",
    "The quality is mediocre at best"
]

for phrase in test_phrases:
    print(f"Text: {phrase}")
    print(f"Prediction: {predict_sentiment(phrase)}\n")

pip install lightgbm

from lightgbm import LGBMClassifier
from sentence_transformers import SentenceTransformer
from sklearn.utils import compute_class_weight
import numpy as np
model = SentenceTransformer('all-mpnet-base-v2')  # ~90MB, very fast

X_train = model.encode(train['Content'].tolist(), show_progress_bar=True)
X_test = model.encode(test['Content'].tolist(), show_progress_bar=True)
class_weights = compute_class_weight('balanced', classes=np.unique(train['sentiment_label']), y=train['sentiment_label'])
class_weight_dict = dict(zip(np.unique(train['sentiment_label']), class_weights))
lgb = LGBMClassifier(class_weight=class_weight_dict, random_state=42)  # Same as before
lgb.fit(X_train, train['sentiment_label'])
test['lgb_pred'] = lgb.predict(X_test)
print(classification_report(test['sentiment_label'], test['lgb_pred']))

from sklearn.feature_extraction.text import TfidfVectorizer

# Disable normalization to keep all TF-IDF values non-negative
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), norm=None)
X_train = tfidf.fit_transform(train['Content'])
X_test = tfidf.transform(test['Content'])
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(X_train, train['sentiment_label'])
test['nb_pred'] = nb.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(test['sentiment_label'], test['nb_pred']))

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, train['sentiment_label'])
test['lr_pred'] = lr.predict(X_test)

print(classification_report(test['sentiment_label'], test['lr_pred']))

from sklearn.svm import LinearSVC
svc = LinearSVC()
svc.fit(X_train, train['sentiment_label'])
test['svc_pred'] = svc.predict(X_test)

print(classification_report(test['sentiment_label'], test['svc_pred']))

from sklearn.naive_bayes import GaussianNB
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), norm=None)
X_train = tfidf.fit_transform(train['Content'])
X_test = tfidf.transform(test['Content'])
from sklearn.naive_bayes import MultinomialNB
nb = GaussianNB()
nb.fit(X_train, train['sentiment_label'])
test['nb_pred'] = nb.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(test['sentiment_label'], test['nb_pred']))

from sklearn.preprocessing import LabelEncoder

# Encode string labels to integers
le = LabelEncoder()
train_labels_encoded = le.fit_transform(train['sentiment_label'])
test_labels_encoded = le.transform(test['sentiment_label'])

# Train XGBoost with encoded labels
xgb_model.fit(X_train, train_labels_encoded)

# Predict and decode predictions
test['xgb_pred_encoded'] = xgb_model.predict(X_test)
test['xgb_pred'] = le.inverse_transform(test['xgb_pred_encoded'])

# Evaluate
from sklearn.metrics import classification_report
print(classification_report(test['sentiment_label'], test['xgb_pred']))

from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report

# Encode labels
le = LabelEncoder()
y_train = le.fit_transform(train['sentiment_label'])
y_test = le.transform(test['sentiment_label'])

# Base model
xgb = XGBClassifier(objective='multi:softmax', num_class=3, use_label_encoder=False, eval_metric='mlogloss', random_state=42)

# Grid of hyperparameters to tune
param_grid = {
    'learning_rate': [0.01, 0.1, 0.3],
    'max_depth': [4, 6, 8],
    'n_estimators': [100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Grid search with cross-validation
grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid,
                           cv=3, scoring='accuracy', n_jobs=-1, verbose=2)

# Fit the model
grid_search.fit(X_train, y_train)

# Get best model
best_model = grid_search.best_estimator_

# Predict and decode predictions
y_pred = best_model.predict(X_test)
y_pred_labels = le.inverse_transform(y_pred)

# Evaluation
print("Best Parameters:", grid_search.best_params_)
print(classification_report(test['sentiment_label'], y_pred_labels))

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, train['sentiment_label'])
test['rf_pred'] = rf.predict(X_test)

print(classification_report(test['sentiment_label'], test['rf_pred']))

pip install catboost

from catboost import CatBoostClassifier
from sklearn.metrics import classification_report

# Label encoding if your labels are still strings
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(train['sentiment_label'])
y_test = le.transform(test['sentiment_label'])

# Create and fit the CatBoost model
cat_model = CatBoostClassifier(
    iterations=200,
    learning_rate=0.1,
    depth=6,
    verbose=0,
    random_state=42
)

cat_model.fit(X_train, y_train)

# Make predictions
y_pred = cat_model.predict(X_test)

# Decode predictions back to labels
y_pred_labels = le.inverse_transform(y_pred)

# Print classification report
print(classification_report(test['sentiment_label'], y_pred_labels))

#CNN
import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report

# ----------------------------
# 1. Tokenize and prepare data
# ----------------------------
vocab_size = 10000
max_len = 100
embedding_dim = 128

tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(train['Content'])

X_train_seq = tokenizer.texts_to_sequences(train['Content'])
X_test_seq = tokenizer.texts_to_sequences(test['Content'])

X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')

# ----------------------------
# 2. Encode labels
# ----------------------------
le = LabelEncoder()
y_train = le.fit_transform(train['sentiment_label'])
y_test = le.transform(test['sentiment_label'])

# ----------------------------
# 3. Build CNN model
# ----------------------------
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),
    Conv1D(128, kernel_size=5, activation='relu'),
    GlobalMaxPooling1D(),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dense(3, activation='softmax')  # Change to match number of classes
])

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# ----------------------------
# 4. Train model
# ----------------------------
model.fit(X_train_pad, y_train, validation_split=0.1, epochs=5, batch_size=32)

# ----------------------------
# 5. Evaluate
# ----------------------------
y_pred = model.predict(X_test_pad)
y_pred_labels = np.argmax(y_pred, axis=1)

print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred_labels)))

import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from scikeras.wrappers import KerasClassifier  # Updated import
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
# ----------------------------
# 1. Tokenize and prepare data
# ----------------------------
vocab_size = 10000
max_len = 100
embedding_dim = 128

tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(train['Content'])

X_train_seq = tokenizer.texts_to_sequences(train['Content'])
X_test_seq = tokenizer.texts_to_sequences(test['Content'])

X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')

# ----------------------------
# 2. Encode labels
# ----------------------------
le = LabelEncoder()
y_train = le.fit_transform(train['sentiment_label'])
y_test = le.transform(test['sentiment_label'])

# ----------------------------
# 3. Define model builder
# ----------------------------
def create_model(optimizer='adam', dropout_rate=0.5):
    model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),
        Conv1D(128, kernel_size=5, activation='relu'),
        BatchNormalization(),
        GlobalMaxPooling1D(),
        Dropout(dropout_rate),
        Dense(64, activation='relu'),
        Dropout(dropout_rate),
        Dense(3, activation='softmax')  # 3 classes
    ])
    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# ----------------------------
# 4. Setup KerasClassifier (SciKeras)
# ----------------------------
early_stopping = EarlyStopping(monitor='val_loss', patience=3)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, min_lr=1e-6)

from sklearn.metrics import accuracy_score

# Define hyperparameter combinations manually
param_combinations = [
    {"batch_size": 16, "epochs": 5, "optimizer": "adam", "dropout_rate": 0.3},
    {"batch_size": 32, "epochs": 10, "optimizer": "sgd", "dropout_rate": 0.5},
    # Add more combinations as needed
]

best_score = -1
best_params = {}

for params in param_combinations:
    print(f"\nTesting params: {params}")

    # Create and train model
    model = create_model(
        optimizer=params["optimizer"],
        dropout_rate=params["dropout_rate"]
    )

    model.fit(
        X_train_pad, y_train,
        batch_size=params["batch_size"],
        epochs=params["epochs"],
        validation_split=0.1,
        verbose=0
    )

    # Evaluate
    y_pred = np.argmax(model.predict(X_test_pad), axis=1)
    score = accuracy_score(y_test, y_pred)

    if score > best_score:
        best_score = score
        best_params = params

print(f"\nBest params: {best_params}, Accuracy: {best_score:.4f}")

#best_model = grid_result.best_estimator_
#y_pred_proba = best_model.predict_proba(X_test_pad)
#y_pred_labels = np.argmax(y_pred_proba, axis=1)

#print(classification_report(y_test, y_pred_labels))

#final_model
#CNN with keras search
import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from kerastuner import HyperModel, RandomSearch
from tensorflow.keras.callbacks import EarlyStopping

# ----------------------------
# 1. Load and Prepare Data
# ----------------------------
# Assuming you have train/test DataFrames with 'Content' and 'sentiment_label'
# If not, replace with your data loading logic:
# train = pd.read_csv('your_data.csv')

# Tokenization
vocab_size = 10000
max_len = 100
embedding_dim = 128

tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(train['Content'])

X_train_seq = tokenizer.texts_to_sequences(train['Content'])
X_test_seq = tokenizer.texts_to_sequences(test['Content'])

X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')

# Label Encoding
le = LabelEncoder()
y_train = le.fit_transform(train['sentiment_label'])
y_test = le.transform(test['sentiment_label'])

# ----------------------------
# 2. Define HyperModel
# ----------------------------
class SentimentHyperModel(HyperModel):
    def build(self, hp):
        model = Sequential([
            Embedding(input_dim=vocab_size,
                      output_dim=hp.Int('embedding_dim', min_value=64, max_value=256, step=64),
                      input_length=max_len),

            Conv1D(filters=hp.Int('conv_filters', min_value=64, max_value=256, step=64),
                   kernel_size=hp.Choice('kernel_size', values=[3, 5, 7]),
                   activation='relu',
                   padding='same'),

            GlobalMaxPooling1D(),

            Dense(units=hp.Int('dense_units', min_value=32, max_value=128, step=32),
                  activation='relu'),

            Dropout(rate=hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1)),

            Dense(3, activation='softmax')
        ])

        optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])
        model.compile(optimizer=optimizer,
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])
        return model

# ----------------------------
# 3. Initialize Tuner
# ----------------------------
tuner = RandomSearch(
    SentimentHyperModel(),
    objective='val_accuracy',
    max_trials=10,  # Number of hyperparameter combinations to try
    executions_per_trial=2,  # Run each trial twice for reliability
    directory='keras_tuner',
    project_name='sentiment_analysis'
)

# Early stopping callback
early_stop = EarlyStopping(monitor='val_loss', patience=3)

# ----------------------------
# 4. Run Hyperparameter Search
# ----------------------------
tuner.search(X_train_pad, y_train,
             epochs=20,
             validation_split=0.2,
             callbacks=[early_stop],
             verbose=2)

# ----------------------------
# 5. Retrieve Best Model
# ----------------------------
# Get the top 2 models
best_models = tuner.get_best_models(num_models=2)
best_model = tuner.get_best_models(num_models=1)[0]

# Get the best hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
Best hyperparameters:
- Embedding dim: {best_hps.get('embedding_dim')}
- Conv filters: {best_hps.get('conv_filters')}
- Kernel size: {best_hps.get('kernel_size')}
- Dense units: {best_hps.get('dense_units')}
- Dropout rate: {best_hps.get('dropout_rate')}
- Optimizer: {best_hps.get('optimizer')}
""")

# ----------------------------
# 6. Evaluate on Test Set
# ----------------------------
# Train final model with best hyperparameters
history = best_model.fit(
    X_train_pad, y_train,
    batch_size=32,
    epochs=20,
    validation_split=0.2,
    callbacks=[early_stop],
    verbose=1
)

# Evaluate
test_loss, test_acc = best_model.evaluate(X_test_pad, y_test)
print(f"\nTest Accuracy: {test_acc:.4f}")

# Generate predictions
y_pred = np.argmax(best_model.predict(X_test_pad), axis=1)
print(classification_report(y_test, y_pred))

!pip install keras-tuner

import pickle
from tensorflow.keras.models import save_model

# Save the trained model
save_model(best_model, 'flipkart_sentiment_model.keras')  # or .h5

# Save the tokenizer (critical for text preprocessing)
with open('tokenizer.pkl', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

# Save the label encoder (if you did class encoding)
with open('label_encoder.pkl', 'wb') as handle:
    pickle.dump(le, handle, protocol=pickle.HIGHEST_PROTOCOL)

from flask import Flask, request, jsonify
import numpy as np
import pickle
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

app = Flask(__name__)

# Load artifacts
model = load_model('flipkart_sentiment_model.keras')
with open('tokenizer.pkl', 'rb') as handle:
    tokenizer = pickle.load(handle)
with open('label_encoder.pkl', 'rb') as handle:
    le = pickle.load(handle)

MAX_LEN = 100  # Same as during training

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # Get review text from POST request
        data = request.get_json()
        text = data['text']

        # Preprocess
        seq = tokenizer.texts_to_sequences([text])
        padded = pad_sequences(seq, maxlen=MAX_LEN)

        # Predict
        pred = model.predict(padded)
        sentiment = le.inverse_transform([np.argmax(pred)])[0]
        confidence = float(np.max(pred))

        return jsonify({
            'sentiment': sentiment,
            'confidence': confidence,
            'text': text
        })
    except Exception as e:
        return jsonify({'error': str(e)})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)

from flask import Flask, request, jsonify
from flask_ngrok import run_with_ngrok
import pickle
import numpy as np
from tensorflow.keras.models import load_model

app = Flask(__name__)
run_with_ngrok(app)  # Start ngrok when app is run

# Load model
model = load_model('/content/flipkart_sentiment_model.keras')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # Get review text from POST request
        data = request.get_json()
        text = data['text']

        # Preprocess
        seq = tokenizer.texts_to_sequences([text])
        padded = pad_sequences(seq, maxlen=MAX_LEN)

        # Predict
        pred = model.predict(padded)
        sentiment = le.inverse_transform([np.argmax(pred)])[0]
        confidence = float(np.max(pred))

        return jsonify({
            'sentiment': sentiment,
            'confidence': confidence,
            'text': text
        })
    except Exception as e:
        return jsonify({'error': str(e)})

app.run()

from flask import Flask, request, jsonify
from flask_ngrok import run_with_ngrok
import numpy as np
import pickle
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load necessary pre-trained components
with open('/content/tokenizer.pkl', 'rb') as handle:
    tokenizer = pickle.load(handle)

with open('/content/label_encoder.pkl', 'rb') as handle:
    le = pickle.load(handle)

MAX_LEN = 100  # Set according to your training setup

# Load model
model = load_model('/content/flipkart_sentiment_model.keras')

# Create Flask app
app = Flask(__name__)
run_with_ngrok(app)  # Start ngrok tunnel when the app is run

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # Get input from POST request
        data = request.get_json()
        text = data['text']

        # Preprocess input
        seq = tokenizer.texts_to_sequences([text])
        padded = pad_sequences(seq, maxlen=MAX_LEN)

        # Predict
        pred = model.predict(padded)
        sentiment = le.inverse_transform([np.argmax(pred)])[0]
        confidence = float(np.max(pred))

        return jsonify({
            'sentiment': sentiment,
            'confidence': confidence,
            'text': text
        })

    except Exception as e:
        return jsonify({'error': str(e)})

# Run Flask app with ngrok tunnel
app.run()



!pip install flask-ngrok # Install the flask_ngrok package

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from kerastuner import HyperModel, RandomSearch

# 1. Enhanced HyperModel Definition
class OptimizedSentimentHyperModel(HyperModel):
    def build(self, hp):
        model = Sequential([
            # Smaller embedding dimension
            Embedding(input_dim=vocab_size,
                     output_dim=hp.Int('embedding_dim', 64, 128, step=32),
                     input_length=max_len),

            # Fewer filters with regularization
            Conv1D(
                filters=hp.Int('conv_filters', 64, 128, step=32),
                kernel_size=hp.Choice('kernel_size', [3, 5]),
                activation='relu',
                padding='same',
                kernel_regularizer='l2'  # Added L2 regularization
            ),

            GlobalMaxPooling1D(),

            # Smaller dense layer with higher dropout
            Dense(
                units=hp.Int('dense_units', 32, 64, step=16),
                activation='relu',
                kernel_regularizer='l2'
            ),

            # Increased dropout rate
            Dropout(rate=hp.Float('dropout_rate', 0.5, 0.7, step=0.1)),

            Dense(3, activation='softmax')
        ])

        optimizer = hp.Choice('optimizer', ['adam', 'rmsprop'])
        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model

# 2. Tuner Setup with Early Stopping
tuner = RandomSearch(
    OptimizedSentimentHyperModel(),
    objective='val_accuracy',
    max_trials=15,
    executions_per_trial=2,
    directory='optimized_tuning',
    project_name='sentiment_analysis_v2'
)

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True  # Critical for getting best model
)

# 3. Run Training with Validation Split
tuner.search(
    X_train_pad, y_train,
    epochs=50,  # Higher but will stop early
    batch_size=32,
    validation_split=0.3,  # Increased validation size
    callbacks=[early_stop],
    verbose=2
)

# 4. Retrieve and Evaluate Best Model
best_model = tuner.get_best_models(num_models=1)[0]
best_hps = tuner.get_best_hyperparameters()[0]

# Evaluate
test_loss, test_acc = best_model.evaluate(X_test_pad, y_test, verbose=0)
print(f"\nOptimized Model Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

# 5. Save the Optimized Model
best_model.save('optimized_sentiment_model.keras')

# Classification Report
y_pred = np.argmax(best_model.predict(X_test_pad), axis=1)
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=le.classes_))

!pip install keras-tuner # Install the keras-tuner package

!pip install --upgrade scikit-learn scikeras

import scikeras
import sklearn
import tensorflow as tf

print("SciKeras version:", scikeras.__version__)
print("Scikit-learn version:", sklearn.__version__)
print("TensorFlow version:", tf.__version__)



!pip install --upgrade scikit-learn

pip install scikeras

#SBERT FLLOWED BY XGBOOST
from sentence_transformers import SentenceTransformer
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import pandas as pd

# Example data (replace with your actual train/test sets)
# train and test should have 'text' and 'sentiment_label' columns
# train = pd.read_csv(...)
# test = pd.read_csv(...)

# 1. Encode sentiment labels
le = LabelEncoder()
train_labels = le.fit_transform(train['sentiment_label'])
test_labels = le.transform(test['sentiment_label'])

# 2. Use SBERT to generate embeddings
sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
X_train = sbert_model.encode(train['Content'].tolist(), show_progress_bar=True)
X_test = sbert_model.encode(test['Content'].tolist(), show_progress_bar=True)

# 3. Train XGBoost classifier
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb_model.fit(X_train, train_labels)

# 4. Predict and evaluate
y_pred = xgb_model.predict(X_test)
y_pred_labels = le.inverse_transform(y_pred)
true_labels = le.inverse_transform(test_labels)

# 5. Classification report
print(classification_report(true_labels, y_pred_labels))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenization
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(train['Content'])
X_train_seq = tokenizer.texts_to_sequences(train['Content'])
X_test_seq = tokenizer.texts_to_sequences(test['Content'])

# Padding
max_len = 100
X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')

# Encode labels
le = LabelEncoder()
y_train = le.fit_transform(train['sentiment_label'])
y_test = le.transform(test['sentiment_label'])

# Model
model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=max_len),
    LSTM(64, return_sequences=False),  # or GRU(64)
    Dense(3, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train_pad, y_train, validation_split=0.1, epochs=5)

# Evaluate
y_pred = model.predict(X_test_pad).argmax(axis=1)
print(classification_report(y_test, y_pred))

!pip install fasttext

!pip install --upgrade pymongo certifi

import sys

print(sys.version)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# 1. Prepare Data ---------------------------------------------------
# Convert to FastText format (__label__prefix required)
df_cleaned['fasttext_format'] = df_cleaned.apply(
    lambda row: f"__label__{row['sentiment_label']} {row['Content']}",
    axis=1
)

# Split data (80% train, 20% test)
train, test = train_test_split(df_cleaned, test_size=0.2, random_state=42)
# Sample: Assume you have 'review_text' and 'rating' columns
df = pd.read_csv('trustpilotFlipkartReviews.csv')

# Clean data (optional: remove NAs, filter non-English, etc.)
df.dropna(subset=['Content', 'Rating'], inplace=True)

# Convert to string and categorical class
X = df['Content'].astype(str)
y = df['Rating'].astype(int)  # Ratings should be 1–5

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# TF-IDF Vectorisation
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Model: Logistic Regression
model = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=1000)
model.fit(X_train_tfidf, y_train)

# Predict & Evaluate
y_pred = model.predict(X_test_tfidf)
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from xgboost import XGBClassifier
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE

# Fix class labels to start from 0
y_train_encoded = y_train - 1
y_test_encoded = y_test - 1

# Oversample minority classes
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train_encoded)

# XGBoost model
model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
model.fit(X_train_resampled, y_train_resampled)

# Evaluate
y_pred = model.predict(X_test_tfidf)
print(classification_report(y_test, y_pred+1))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import SMOTE
from collections import Counter

# 1. Load and clean data ---------------------------------------------------
df = pd.read_csv('trustpilotFlipkartReviews.csv')
df.dropna(subset=['Content', 'Rating'], inplace=True)
df['Content'] = df['Content'].astype(str)
df['Rating'] = df['Rating'].astype(int)

# Optional: check class distribution
print("Class distribution before resampling:", Counter(df['Rating']))

# 2. Split the data with stratification ------------------------------------
X = df['Content']
y = df['Rating']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# 3. TF-IDF Vectorisation ---------------------------------------------------
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# 4. Address Class Imbalance with SMOTE ------------------------------------
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)

print("Class distribution after SMOTE:", Counter(y_train_resampled))

# 5. Train the model with balanced weights ----------------------------------
model = LogisticRegression(
    multi_class='multinomial',
    solver='saga',
    max_iter=1000,
    class_weight='balanced'
)
model.fit(X_train_resampled, y_train_resampled)

# 6. Predict and Evaluate ---------------------------------------------------
y_pred = model.predict(X_test_tfidf)

print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# Load and prepare data
df = pd.read_csv('trustpilotFlipkartReviews.csv')
df.dropna(subset=['Content', 'Rating'], inplace=True)
df['Content'] = df['Content'].astype(str)
df['Rating'] = df['Rating'].astype(int)

# Encode labels starting from 0
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['Rating'])  # Now 0 to 4 instead of 1 to 5
num_classes = len(np.unique(y))
y_cat = to_categorical(y, num_classes=num_classes)

# Tokenise text
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(df['Content'])
X_seq = tokenizer.texts_to_sequences(df['Content'])

# Pad sequences
max_len = 200
X_pad = pad_sequences(X_seq, maxlen=max_len)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_pad, y_cat, test_size=0.2, stratify=y, random_state=42)

# Build CNN model
model = Sequential()
model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_len))
model.add(Conv1D(128, kernel_size=5, activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

# Compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train model
model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)

# Evaluate
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\nTest Accuracy: {accuracy:.4f}")
# Predict class probabilities
y_pred_probs = model.predict(X_test)

# Convert probabilities to class labels
y_pred_classes = np.argmax(y_pred_probs, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

# Inverse transform the labels to get original ratings (if needed)
y_pred_labels = label_encoder.inverse_transform(y_pred_classes)
y_true_labels = label_encoder.inverse_transform(y_true_classes)

# Print classification report
print("\nClassification Report:\n")
print(classification_report(y_true_labels, y_pred_labels))

unique_values_count = df_cleaned['Rating'].nunique()
print("Number of unique values in 'ratings':", unique_values_count)

# Count of rows per unique value in 'ratings'
counts = df_cleaned['Rating'].value_counts()
print(counts)

from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.combine import SMOTEENN
from scipy.sparse import hstack
df = pd.read_csv('trustpilotFlipkartReviews.csv')

# Clean data (optional: remove NAs, filter non-English, etc.)
df.dropna(subset=['Content', 'Rating'], inplace=True)

# Convert to string and categorical class
X = df['Content'].astype(str)
y = df['Rating'].astype(int)  # Ratings should be 1–5
# Assuming your text column is named 'review_text'
tfidf = TfidfVectorizer(max_features=5000)  # You can adjust max_features

X_text = tfidf.fit_transform(df['Content'])



# If only text features:
X_combined = X_text

smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_combined, y)
print(y_res.value_counts())

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# 1. Split the resampled data
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42)

# 2. Train a classifier
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# 3. Predict
y_pred = clf.predict(X_test)

# 4. Evaluate
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

import numpy as np

plt.figure(figsize=(12, 3))
plt.plot(np.arange(len(y_test)), y_test, 'bo', label='Actual')
plt.plot(np.arange(len(y_pred)), y_pred, 'rx', label='Predicted')
plt.legend()
plt.title("Actual vs Predicted Labels (Index-wise)")
plt.xlabel("Sample Index")
plt.ylabel("Class Label")
plt.show()

import seaborn as sns
import pandas as pd

df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})

plt.figure(figsize=(10,5))
sns.countplot(x='Actual', data=df, color='blue', alpha=0.6, label='Actual')
sns.countplot(x='Predicted', data=df, color='red', alpha=0.4, label='Predicted')
plt.title("Actual vs Predicted Class Distribution")
plt.legend()
plt.show()
